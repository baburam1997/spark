{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "    .appName(\"Date Column Cleaning\")\\\n",
    "        .master(\"local[2]\")\\\n",
    "            .config(\"spark.executor.memory\", \"2g\")\\\n",
    "                .config(\"spark.driver.memory\", \"1g\")\\\n",
    "                    .config(\"spark.sql.shuffle.partitions\", \"4\")\\\n",
    "                        .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\\\n",
    "                            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions  as F\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"DateFormatsExample\").getOrCreate()\n",
    "\n",
    "# Sample data with a date column in a standard format (yyyy-MM-dd)\n",
    "data = [(\"2023-01-15\",)]\n",
    "df = spark.createDataFrame(data, [\"date_column\"])\n",
    "\n",
    "# Add columns with different date formats\n",
    "df = df.withColumn(\"standard_date\", F.to_date(F.col(\"date_column\"), \"yyyy-MM-dd\"))\n",
    "df = df.withColumn(\"short_date\", F.to_date(F.col(\"date_column\"), \"dd-MM-yy\"))\n",
    "df = df.withColumn(\"long_date\", F.to_date(F.col(\"date_column\"), \"dd MMMM yyyy\"))\n",
    "df = df.withColumn(\"iso_date\", F.to_date(F.col(\"date_column\"), \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\"))\n",
    "df = df.withColumn(\"custom_date\", F.to_date(F.col(\"date_column\"), \"yyyy-MM-dd HH:mm:ss z\"))\n",
    "df = df.withColumn(\"epoch_timestamp\", F.from_unixtime(F.lit(1642185600), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "df = df.withColumn(\"quarter\", F.quarter(F.col(\"standard_date\")))\n",
    "df = df.withColumn(\"year\", F.year(F.col(\"standard_date\")))\n",
    "df = df.withColumn(\"day_of_week\", F.date_format(F.col(\"standard_date\"), \"EEEE\"))\n",
    "df = df.withColumn(\"week_of_year\", F.date_format(F.col(\"standard_date\"), \"w\"))\n",
    "df = df.withColumn(\"month_abbreviation\", F.date_format(F.col(\"standard_date\"), \"MMM\"))\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the options and configurations\n",
    "options = {\n",
    "    \"header\": \"true\",          # Use the first row as the header\n",
    "    \"inferSchema\": \"true\",     # Infer the schema of the CSV file\n",
    "    \"sep\": \",\",                # Specify the separator (default is ',')\n",
    "    \"encoding\": \"UTF-8\",       # Set the character encoding\n",
    "    \"mode\": \"DROPMALFORMED\"    # Handle malformed rows by dropping them\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = 'input-data/date_formats.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file with the specified options\n",
    "df = spark.read.csv(csv_file_path, **options)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_to_standard_date_format(df, to_standard_format=True):\n",
    "    for col in df.columns:\n",
    "        new_col_name = col\n",
    "\n",
    "        if to_standard_format:\n",
    "            df = df.withColumn(new_col_name, F.to_date(F.col(new_col_name), \"yyyy-MM-dd\"))\n",
    "            df = df.withColumnRenamed(col, new_col_name)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply the date format conversion function\n",
    "df = date_to_standard_date_format(df)\n",
    "\n",
    "# Print the schema to see the changes\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
